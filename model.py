# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iWTDwATYSYt51UnN1WnVxOQPBtBrOk-o
"""

#pip install h5py

import h5py
import csv
import numpy as np
import pandas as pd
import tensorflow as tf

from keras.models import Sequential
from keras.layers import Dense, LSTM
from numpy import array
from numpy.random import uniform
from numpy import hstack
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.layers.embeddings import Embedding

"""Getting X"""

dataset = h5py.File('/content/drive/MyDrive/Sem8/ISEAR&Tweets_embeddings.hdf5', 'r') 
#data = dataset.get('vectors').value
data = dataset['vectors'][:]
data.shape

from google.colab import drive
drive.mount('/content/drive')

"""Getting Y"""

df = pd.read_csv('/content/drive/MyDrive/Sem8/EnhancedSentiment140.csv')

y_label = []
# x_tweet = []v
for i in range(50000):
  # x_tweet.append(df.loc[i]['Tweet'])
  emotion = df.loc[i]['Emotion']
  arr = np.array(emotion[1:-1].split(','))
  arr = arr.astype(np.float)
  y_label.append(arr)

x, y = data, np.array(y_label)

print(type(y[0]))
print(x.shape)

x = x.reshape(x.shape[0], x.shape[1], 1)
print("x:", x.shape, "y:", y.shape)

in_dim = (x.shape[1], x.shape[2])
out_dim = 8
print(in_dim)
print(out_dim)

xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.2)
print("xtrain:", xtrain.shape, "ytrian:", ytrain.shape)

"""

```
# This is formatted as code
```

Tweet to embedding
D"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from numpy import array
from numpy import asarray
from numpy import zeros

tokenizer = Tokenizer(num_words=10000)

x_tweet = tokenizer.texts_to_sequences(x_tweet)
vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)

tokenizer.fit_on_texts(x_tweet)

max_len = 100
x_tweet = np.array(pad_sequences(x_tweet, padding='post', maxlen=max_len))

embeddings_dictionary = dict()
glove_file = open('/content/drive/MyDrive/Sem8/glove.6B.100d.txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
glove_file.close()

hits = 0
misses = 0
embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
        hits += 1
    else:
        misses += 1

print("Converted %d words (%d misses)" % (hits, misses))

"""Model"""

model = Sequential()
# embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], 
                            #input_length=max_len , trainable=False)
# model.add(embedding_layer)v
model.add(LSTM(32, input_shape=in_dim))
# model.add(LSTM(100, activation="relu"))
model.add(Dense(out_dim, activation='sigmoid'))
model.compile(loss="mse", optimizer="adam")
model.summary()

checkpoint_filepath = '/content/drive/MyDrive/Sem8/Model'

"""Training"""

hist = model.fit(xtrain, ytrain, epochs=20, batch_size=1, verbose=2)

"""Testing"""

hist.save(checkpoint_filepath + '/my_model2.h5')

ypred = model.predict(xtest)
print("y1 MSE:%.4f" % mean_squared_error(ytest[:,0], ypred[:,0]))
print("y2 MSE:%.4f" % mean_squared_error(ytest[:,1], ypred[:,1]))
print("y3 MSE:%.4f" % mean_squared_error(ytest[:,2], ypred[:,2]))
print("y4 MSE:%.4f" % mean_squared_error(ytest[:,3], ypred[:,3]))
print("y5 MSE:%.4f" % mean_squared_error(ytest[:,4], ypred[:,4]))
print("y6 MSE:%.4f" % mean_squared_error(ytest[:,5], ypred[:,5]))
print("y7 MSE:%.4f" % mean_squared_error(ytest[:,6], ypred[:,6]))
print("y8 MSE:%.4f" % mean_squared_error(ytest[:,7], ypred[:,7]))

x_ax = range(len(xtest))
plt.title("LSTM multi-output prediction")
plt.scatter(x_ax, ytest[:,0],  s=6, label="y1-test")
plt.plot(x_ax, ypred[:,0], label="y1-pred")
plt.scatter(x_ax, ytest[:,1],  s=6, label="y2-test")
plt.plot(x_ax, ypred[:,1], label="y2-pred")
plt.scatter(x_ax, ytest[:,2],  s=6, label="y3-test")
plt.plot(x_ax, ypred[:,2], label="y3-pred")
plt.scatter(x_ax, ytest[:,3],  s=6, label="y4-test")
plt.plot(x_ax, ypred[:,3], label="y4-pred")
plt.scatter(x_ax, ytest[:,4],  s=6, label="y5-test")
plt.plot(x_ax, ypred[:,4], label="y5-pred")
plt.scatter(x_ax, ytest[:,5],  s=6, label="y6-test")
plt.plot(x_ax, ypred[:,5], label="y6-pred")
plt.scatter(x_ax, ytest[:,6],  s=6, label="y7-test")
plt.plot(x_ax, ypred[:,6], label="y7-pred")
plt.scatter(x_ax, ytest[:,7],  s=6, label="y8-test")
plt.plot(x_ax, ypred[:,7], label="y8-pred")
plt.legend()
plt.show()

print(ytest[67])
print(ypred[67])

